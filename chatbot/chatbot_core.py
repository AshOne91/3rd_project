# chatbot/chatbot_core.py

import numpy as np
import config
from rag.vector_store import load_category_vector_db, load_vector_db_by_path
from rag.embedder import load_embedder
from llm.category_classifier import classify_category_with_llm
from llm.router import get_llm_by_category
from llm.responder import build_rag_chain
from llm.chatbot_llm import chatbot_response
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# ì´ˆê¸°í™” (ì„œë²„ ê¸°ë™ì‹œ 1íšŒ)
embedder = load_embedder()
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
path,_ = config.VECTOR_DB_PATHS.get("treatment", config.VECTOR_DB_PATHS["default"])
faiss_db = FAISS.load_local(
    folder_path=path,
    embeddings=embedding_model,
    allow_dangerous_deserialization=True  # ğŸ” ì•ˆì „í•˜ê²Œ ì§ì ‘ ë§Œë“  ê²½ìš°ì—ë§Œ ì‚¬ìš©
)

category_texts, category_categories, category_embeddings = load_category_vector_db()

# FAISS ì¸ë±ìŠ¤
import faiss
index = faiss.IndexFlatL2(category_embeddings.shape[1])
index.add(category_embeddings.numpy())

def run_chatbot_pipeline(user_input: str, session_id: str = "default") -> str:
    # 1. ë¶„ë¥˜ìš© ë²¡í„° ê²€ìƒ‰
    input_emb = embedder.encode([user_input])
    D, I = index.search(np.array(input_emb).astype("float32"), k=3)
    top_sims = 1 - D[0]  # L2 ê±°ë¦¬ â†’ ìœ ì‚¬ë„(ì½”ì‚¬ì¸ ê¸°ë°˜ì´ë¼ë©´ ë³„ë„ ì²˜ë¦¬)
    top_idx = I[0]
    max_sim = top_sims[0]

    if max_sim < 0.01:
        # context ì—†ì´ ë°”ë¡œ ì±—ë´‡ LLMì— ì „ë‹¬
        return chatbot_response(user_input, "")
    else:
    # context í™•ë³´
        retrieved_examples = [(category_texts[i], category_categories[i], float(top_sims[j])) for j, i in enumerate(top_idx)]
        predicted_category = classify_category_with_llm(user_input, retrieved_examples)
    # 3. ì¹´í…Œê³ ë¦¬ë³„ ë²¡í„° DBì—ì„œ ë¬¸ì„œ ì¬ê²€ìƒ‰ (context ì¶”ì¶œ)

    # ë‹¤ë¥¸ë¶„ë“¤ ë²¡í„°dbì˜¤ë©´ ë³‘í•©í•˜ê¸°
    context = ""
    predicted_category = "default"
    if predicted_category == "treatment":
        results = faiss_db.similarity_search(user_input, k=3)
        context = "\n\n".join([doc.page_content for doc in results])
    else:
        index_file, chunks_file = config.VECTOR_DB_PATHS.get(predicted_category, config.VECTOR_DB_PATHS["default"])
        context_docs, context_index = load_vector_db_by_path(index_file, chunks_file)
        doc_emb = embedder.encode([user_input])
        D2, I2 = context_index.search(np.array(doc_emb).astype("float32"), k=3)
        top_docs = [context_docs[i] for i in I2[0]]
        context = "\n".join(top_docs)

    # 4. ì¹´í…Œê³ ë¦¬ë³„ ì „ë¬¸ LLMìœ¼ë¡œ 1ì°¨ ë‹µë³€
    category_llm = get_llm_by_category(predicted_category)
    rag_chain = build_rag_chain(category_llm)
    expert_response = rag_chain.invoke({"question": user_input, "context": context})
    # 5. ìµœì¢… ì±—ë´‡ LLM
    return chatbot_response(user_input, expert_response, session_id=session_id)